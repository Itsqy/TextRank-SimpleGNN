#!/usr/bin/env python3
"""
Test script for manual reference evaluation
"""

def test_manual_reference_flow():
    """Test the manual reference evaluation flow"""
    
    print("ğŸ§ª Testing Manual Reference Evaluation Flow")
    print("=" * 50)
    
    # Simulate the flow
    print("1. âœ… User generates summary")
    print("2. âœ… User selects 'Manual Reference'")
    print("3. âœ… User enters reference summary")
    print("4. âœ… User clicks 'Calculate ROUGE Metrics' button")
    print("5. âœ… App calculates and displays metrics")
    print()
    
    # Test the actual evaluation function
    from rouge_score import rouge_scorer
    
    # Sample data
    generated_summary = "AI has transformed many industries. Machine learning solves complex problems."
    manual_reference = "Artificial intelligence has revolutionized many sectors. Machine learning algorithms solve difficult problems."
    
    print("ğŸ“ Generated Summary:")
    print(f"   {generated_summary}")
    print()
    print("ğŸ“ Manual Reference:")
    print(f"   {manual_reference}")
    print()
    
    # Calculate metrics
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    scores = scorer.score(manual_reference, generated_summary)
    
    print("ğŸ“Š ROUGE Metrics Results:")
    for metric_name, score in scores.items():
        print(f"   {metric_name.upper()}:")
        print(f"     Precision: {score.precision:.3f}")
        print(f"     Recall:    {score.recall:.3f}")
        print(f"     F1-Score:  {score.fmeasure:.3f}")
        print()
    
    print("ğŸ‰ Manual reference evaluation test completed!")
    print("ğŸ’¡ The feature should now work properly in the Streamlit app.")

if __name__ == "__main__":
    test_manual_reference_flow() 